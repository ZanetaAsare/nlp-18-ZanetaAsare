{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "filelist = ['yelp_labelled.txt','imdb_labelled.txt','amazon_cells_labelled.txt']\n",
    "data = [pd.read_table(file, sep ='\\t',header=None,names=['sentence','class']) for file in filelist]\n",
    "\n",
    "big_data = pd.concat(data)\n",
    "\n",
    "big_data['sentence'] = big_data['sentence'].apply(nltk.word_tokenize)          #performing word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing word normalization\n",
    "\n",
    "big_data['sentence'] = big_data.sentence.map(lambda x: x.lower())            #converting all words to lowercase\n",
    "big_data['sentence'] = big_data.sentence.str.replace('[^\\w\\s]', '')           #removing punctuations from sentences\n",
    "\n",
    "\n",
    "big_data['sentence'] = big_data['sentence'].apply(nltk.word_tokenize)          #performing word tokenization\n",
    "\n",
    "#performing word stemming\n",
    "stemmer = PorterStemmer()                                                        \n",
    "big_data['sentence'] = big_data['sentence'].apply(lambda x: [stemmer.stem(y) for y in x]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                             Wow ... Loved this place .\n",
      "1                                    Crust is not good .\n",
      "2             Not tasty and the texture was just nasty .\n",
      "3      Stopped by during the late May bank holiday of...\n",
      "4      The selection on the menu was great and so wer...\n",
      "5        Now I am getting angry and I want my damn pho .\n",
      "6               Honeslty it did n't taste THAT fresh . )\n",
      "7      The potatoes were like rubber and you could te...\n",
      "8                             The fries were great too .\n",
      "9                                        A great touch .\n",
      "10                             Service was very prompt .\n",
      "11                                   Would not go back .\n",
      "12     The cashier had no care what so ever on what I...\n",
      "13     I tried the Cape Cod ravoli , chicken , with c...\n",
      "14     I was disgusted because I was pretty sure that...\n",
      "15     I was shocked because no signs indicate cash o...\n",
      "16                                  Highly recommended .\n",
      "17               Waitress was a little slow in service .\n",
      "18     This place is not worth your time , let alone ...\n",
      "19                                 did not like at all .\n",
      "20                                  The Burrittos Blah !\n",
      "21                                  The food , amazing .\n",
      "22                                Service is also cute .\n",
      "23     I could care less ... The interior is just bea...\n",
      "24                                   So they performed .\n",
      "25     That 's right ... .the red velvet cake ... ..o...\n",
      "26           - They never brought a salad we asked for .\n",
      "27     This hole in the wall has great Mexican street...\n",
      "28     Took an hour to get our food only 4 tables in ...\n",
      "29                    The worst was the salmon sashimi .\n",
      "                             ...                        \n",
      "970    I plugged it in only to find out not a darn th...\n",
      "971                                  Excellent product .\n",
      "972                         Earbud piece breaks easily .\n",
      "973                                      Lousy product .\n",
      "974    This phone tries very hard to do everything bu...\n",
      "975    It is the best charger I have seen on the mark...\n",
      "976                                 SWEETEST PHONE ! ! !\n",
      "977          : - ) Oh , the charger seems to work fine .\n",
      "978    It fits so securely that the ear hook does not...\n",
      "979                                  Not enough volume .\n",
      "980                Echo Problem ... .Very unsatisfactory\n",
      "981    you could only take 2 videos at a time and the...\n",
      "982                            do n't waste your money .\n",
      "983    I am going to have to be the first to negative...\n",
      "984    Adapter does not provide enough charging curre...\n",
      "985    There was so much hype over this phone that I ...\n",
      "986    You also can not take pictures with it in the ...\n",
      "987                             Phone falls out easily .\n",
      "988    It did n't work , people can not hear me when ...\n",
      "989    The text messaging feature is really tricky to...\n",
      "990    I 'm really disappointed all I have now is a c...\n",
      "991                                 Painful on the ear .\n",
      "992                    Lasted one day and then blew up .\n",
      "993                                       disappointed .\n",
      "994                               Kind of flops around .\n",
      "995    The screen does get smudged easily because it ...\n",
      "996    What a piece of junk.. I lose more calls on th...\n",
      "997                        Item Does Not Match Picture .\n",
      "998    The only thing that disappoint me is the infra...\n",
      "999    You can not answer calls with the unit , never...\n",
      "Name: sentence, Length: 2748, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#This section is used for feature extraction\n",
    "# This converts the list of words into space-separated strings\n",
    "big_data['sentence'] = big_data['sentence'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "print(big_data['sentence'])\n",
    "\n",
    "vectorizer = CountVectorizer()                                             #an algorithm for extracting the features\n",
    "counts = vectorizer.fit_transform(big_data['sentence'])\n",
    "\n",
    "\n",
    "#this is for assigning weights to words in terms of frequency and importance\n",
    "transformer = TfidfTransformer().fit(counts)\n",
    "counts = transformer.transform(counts) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                 This GPS tracker works like a charm .\n",
      "1     When I opened the box the product was not in t...\n",
      "2          Everyone should have one who owns a computer\n",
      "3                                    Buy something else\n",
      "4     Pure junk do not buy ever the greatest load of...\n",
      "5        The DataVac was used and full of dust and dirt\n",
      "6     Not so great ... bought to clean the bobbin ca...\n",
      "7     It is a great size , I keep it in my desk draw...\n",
      "8     I just bought this Vacuum . It 's just good fo...\n",
      "9     This is just perfect for vacuuming out the lin...\n",
      "10    I use it mostly to vacuum threads on the sewin...\n",
      "11    I have found this mini vac . to be everything ...\n",
      "12    I ordered the Pork Prime Rib Chop it was beaut...\n",
      "13    A bastion of fine dining in The City for 20 ye...\n",
      "14    Took my brand new bmw in for service . When I ...\n",
      "15    I 'll never buy another car from this location...\n",
      "16    Service Department , once the crown jewel of t...\n",
      "17    Kevin is very friendly , accommodating and has...\n",
      "18    Had a great experience leasing a new car here ...\n",
      "19    Following the showing we were all dissapointed...\n",
      "20                        Shame I ca n't rate this 0/10\n",
      "21    This movie has it all , Great acting , action ...\n",
      "22                                Total waste of time .\n",
      "23    If this is critically acclaimed and highly rat...\n",
      "24    Clearly , Black Panther represents a great ste...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#vectorizing the test file\n",
    "testfile = 'test_sentences.txt'\n",
    "\n",
    "test_data = pd.read_table(testfile, sep='\\t',header=None,names=['sentence'])\n",
    "\n",
    "test_data['sentence'] = test_data['sentence'].apply(nltk.word_tokenize)\n",
    "\n",
    "test_data['sentence'] = test_data['sentence'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "#an algorithm for extracting the features\n",
    "newcount = vectorizer.fit_transform(test_data['sentence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-351233dc5fcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#this section is used for evaluating the naive bayes classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \"\"\"\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mjll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 725\u001b[1;33m         return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n\u001b[0m\u001b[0;32m    726\u001b[0m                 self.class_log_prior_)\n\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \"\"\"\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"toarray\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dimension mismatch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "#this section is used for training the naive bayes classifier\n",
    "\n",
    "#the data is first split into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts, big_data['class'], test_size=0.1, random_state=69)\n",
    "\n",
    "#the training set is used to train the classifier\n",
    "model = MultinomialNB().fit(X_train, y_train) \n",
    "\n",
    "#this section is used for evaluating the naive bayes classifier\n",
    "\n",
    "predicted = model.predict(newcount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this section is used for training the logistic regression classifier\n",
    "\n",
    "#the data is first split into training and testing data\n",
    "x_train, x_test, y_train, y_test = train_test_split(counts, big_data['class'], test_size=0.1, random_state=69)\n",
    "\n",
    "logisticRegr = LogisticRegression()\n",
    "\n",
    "#the actual training\n",
    "model = logisticRegr.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "#this section is used for evaluating the logistic regression classifier\n",
    "\n",
    "predicted = model.predict(x_test)\n",
    "print(accuracy_score(y_test,predicted) * 100) \n",
    "print(confusion_matrix(y_test, predicted)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = open(\"results.txt\", \"w\")\n",
    "\n",
    "for val in predicted:\n",
    "    result.write(str(val) + '\\n')\n",
    "    \n",
    "result.close()\n",
    "print('Check current folder for your results file')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
